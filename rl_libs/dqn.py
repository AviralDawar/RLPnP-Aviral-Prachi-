# -*- coding: utf-8 -*-
"""DQN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNq0P24Ky7R-NPuUPPPu4uutmppzWSTZ
"""

# an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments
import gym
import numpy as np
import pandas as pd
from stable_baselines import DQN
from gym import spaces 
import copy
import math
import csv
import os

print("hi")

def results_into_file(df, action_list, reward_list):
   
    ls  = list(df.columns.values)
    ls.append('Action')
    ls.append('Reward')
    
    print ("PRINTING")
    print (ls)
    print (len(ls))
    print (len(df))

    
    df = df.to_numpy()
    with open('DQN_output.csv', 'w') as csvoutput:
        csvwriter = csv.writer(csvoutput)
        csvwriter.writerow(ls)
        for i in range(0,len(df)):
            temp = []
            for j in range(0,len(ls)-2):
                print (i, j)
                temp.append(df[i][j])

            if(i >= len(action_list)):
                temp.append(0)
                temp.append(0)
            else:
                temp.append(action_list[i])
                temp.append(reward_list[i])
            csvwriter.writerow(temp)

    return 'DQN_Output.csv'

# creating an environment: our custom environment, built to solve our problem
class StatesEnv(gym.Env):
	# mode of render function: human indicates that output on console will be understandable
	metadata = {'render.modes':['human']}

	# initialization function: initializes the class and initial state of our problem
	def __init__(self, s, episodes, total, features,reward, buckets, total_reward, total_resources):    #susceptible ratio is ratio of susceptible people in that state/total susceptible people combining all states

		self.states = s #no of states

		
		self.observation_space = spaces.Box(np.zeros(len(features)+1), np.ones(len(features)+1), shape=(len(features)+1,), dtype = np.float32)	#continuous observation space
		# Action space is a discrete valued space. number of actions = number of buckets
		self.action_space = spaces.Discrete(buckets)	#discrete action space

		self.curr_step = 1
		self.done = False
		
		# (5x100 matrix)
		self.valueMap = np.zeros((self.states, 100))
		self.total = total #total number of vials available in 1 batch = batch size 
		self.episodes = episodes

		
		self.received = [0]*self.states

		
		self.states_cond = []
		self.action_list = None

		
		self.susc = [0]*self.states
		# self.confirmed=confirmed
		# self.dr=dr
		# self.rr=rr 
		# self.population=population
		self.reward=reward
		# self.bed=bed
		# self.icu=icu
		# self.ventilator=ventilator
		self.features_list = features
		self.features_list.append(reward)

		self.total_resources = total_resources
		self.buckets=buckets
		self.total_reward=total_reward
		self.reset()

	# function to return integer casted n
	def get_discrete_int(self, n):
		discrete_int = int(n)
		return discrete_int

	# resets the state ans other variables of the environment to the start state
	def reset(self):
		self.curr_step = 1
		self.done = False

		# we are starting with 10 lakh vaccines to distribute
		self.total = self.total_resources          #10 lakh vaccines

		# array of all values of features 
		self.states_cond =  np.array(self.features_list)

		# returning a copy of the array
		return copy.deepcopy(self.states_cond)
        
	# step function: takes an action variable and return a list of 4 things (next state, reward for the current state, whether episode is done, etc..additional info) 
	def step(self, action):
		# action was set to None at the beginning	
		self.action_list = action

		# gets reward
		reward = self.get_reward()

		# ratio of action to total number of actions 
		vaccine_allotted = (action/self.buckets)*self.total

		# new_susceptibility ratio after vaccine allotment 
		new_reward = ((self.reward*self.total_reward)-vaccine_allotted)/(self.total_reward-vaccine_allotted)

		# setting new susceptible ratio
		self.states_cond[len(self.features_list)-1] = new_reward

		# increment episode
		if self.curr_step == self.episodes:
			# done with all episodes
			self.done=True
		else:
			# else increment to next episode
			self.done=False
			self.curr_step+=1
        
		return self.states_cond, reward, self.done, {'action_list': self.action_list, 'distribution':(self.action_list*(1/self.buckets))+(0.5/self.buckets), 'episode_number': self.curr_step}

   	# function to return reward 
	def get_reward(self):
		reward = 0              

		# dividing action values by number of actions that can be taken
		A=(self.action_list*(1/self.buckets))
      # //summation of all the features as a default 

		# this is susceptible ratio??
		S=self.states_cond[len(self.features_list)-1]                                                    

		
		reward = (100 * math.exp((-(A-S)**2)))
		return reward 

	def close(self):
        	pass

#Pass input
def DQN_run(inp):
	print("hi I'm in dqn.py")
	
	print(inp)
	print (os.path.abspath(inp.file_path))
	df=pd.read_csv(inp.file_path,skipinitialspace=True,index_col=None)
	print (df)
	
	states = df[inp.states].unique()
	num_states = len(states)
	# empty action list and reward list
	action_list=[]
	reward_list=[]
	
	attr = []
	no = inp.features

	for i in range (0,no):
		ele = inp.attributes[i]
		print(ele)
		attr.append(df.columns.get_loc(ele))
		
	# iterating through file
	for i in range(0,75,num_states):   # change according to how many days the model needs to be trained
		arr=df[i:i+num_states].to_numpy()    #i to i+5 for 5 states on single date
		# summing up all 5 row values for each column: gives you total for the day
		r = df.columns.get_loc(inp.indicator)
		total_reward=sum(arr[:,r])
		# total_old=sum(arr[:,10])
		ttsm = []
		for i in range(0,inp.features):
			ttsm.append(sum(arr[:,attr[i]]))
			# iterating through each state
		for s in range(num_states):          # looping for 5 states
			values = []
			for i in range(0,inp.features):
				values.append(arr[s][attr[i]]/ttsm[i])
			# changing susceptible cases to ration of susceptible/total susceptible for the day
			reward = arr[s][r]/total_reward
			# old_ratio=arr[s][10]/total_old
			env=StatesEnv(1,1,inp.episodes,values,reward,inp.bucket_size, total_reward,inp.total_resources)
			
			# DQN model
			model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=1, exploration_fraction=0.2)
			print (model)
			model.learn(total_timesteps=inp.epochs, log_interval=inp.epochs)
			obs = env.reset()
			print (obs)
			action, _states = model.predict(obs)
			obs, reward, done, info = env.step(action)
			action_list.append(action)
			reward_list.append(reward)
	results_into_file(df,action_list, reward_list)

	return action_list,reward_list 

