# -*- coding: utf-8 -*-
"""Discretized_RLPnP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmP_ywqYP7MaGmsjjbUxE97KJV6Hpb2W
"""

import numpy as np 
import gym 
from gym import spaces 
import matplotlib.pyplot as plt 
import math
import random

from stable_baselines3 import A2C, DQN

from sklearn.preprocessing import KBinsDiscretizer
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler  
import pandas as pd
import csv
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt

# Environment
class StatesEnv(gym.Env):
    """
    Customised Environment that follows gym interface.
    Describes relevant properties of the state and action spaces. 
    """

    # Human friendly output
    metadata = {'render.modes':['human']}
    
    
    # Initialize
    def __init__(self, s, episodes, total,states, discrete_cc, affected_feature):

        self.states = s #no of independent simulations to be run 
        self.affected_feature = affected_feature
        
        # lower limit of confirmed and death
        low = np.ones((s,3))
        # higher limit of confirmed and death
        high = np.ones((s,3)) * 100

        # observation_space = states (grid)
        self.observation_space = spaces.Box(low, high, shape=(s, 3), dtype = np.float)

        # actions are vectors of the form [n1, n2, n3,...nk] for k states 
        self.action_space = spaces.Box(low = np.zeros((s), dtype = int), high = np.ones((s), dtype = int)* total)

        # # Action space is vector
        # self.action_space = spaces.Discrete(total)

        # starts at step = 0
        self.curr_step = 0

        # done set to False at the beginning
        self.done = False
        
        self.valueMap = np.zeros((self.states, 100))
        # total number of resources available
        self.total = total #total number of vials available in 1 batch = batch size 
        
        self.fixed_total = total
        # number of episodes
        self.episodes = episodes
        # number of received resources by each state
        self.received = [0]*self.states
        self.states_cond = []
        self.action_list = []
        self.gamma = 0.20
        self.epsilon = 1.0
        # init min reward
        self.min_reward = -10000000
        self.min_reward_seq = []
        self.best_actions = []
        self.reward_list = []
        self.st_ates = self.states
        self.non_transformed = []

        self.discrete_cc = discrete_cc
        self.running_reward_list = []
            
    def get_discrete_int(self, n):
        discrete_int = int(n)
        return discrete_int

    def reset(self):
        """
        Resets observation_space to a matrix initialising situation of states wrt the current figures; 
        action_space tp start exploring from the point of equal distribution between all states.
        """
        
        self.curr_step = 0
        self.cn = 0
        self.done = False
        self.total = self.fixed_total
        # Declare the Initial Conditions for the States
       
        self.states_cond =  np.array(self.st_ates)
        #store the actions in an array 
        self.action_list = np.array([self.fixed_total/(self.states)]*(self.states))

        return self.states_cond
        

    def step(self, action): 
        # check if we're done
        if self.curr_step >= self.episodes - 1:
            self.done = True
        # print("Are we done?", self.done)
        
        if(self.cn == self.episodes/10):
          self.cn = 0 
          self.epsilon = self.epsilon/10    
        if self.states_cond is None:
            raise Exception("You need to reset() the environment before calling step()!")
       
        #start with equal distribution 
        if self.curr_step == 1:
          self.action_list = np.array([self.fixed_total/(self.states)])*(self.states)
          self.reward_list = []
        else:
          self.action_list = action

        #exploration vs exploitation        
        if random.uniform(0, 1) < self.epsilon:
            self.action_list = np.random.randint(0, self.fixed_total, self.states)
        else:
            self.action_list = action

        print ("Action: ", self.action_list)

        # Proportion
        sum_of_actions = sum(self.action_list)
        if(sum_of_actions == 0):
          sum_of_actions = 1
        self.action_list = self.action_list/sum_of_actions
        self.non_transformed = self.action_list

        temp_actions = self.action_list.reshape((self.action_list.shape[0], 1))


        #no of units distrbuted to respective states              
        for i in range(self.states):
            self.received[i] = self.action_list[i]
             

        #reward only when task done 
        reward = self.get_reward()

        # increment episode
        self.curr_step += 1

        return self.states_cond, reward, self.done, {'action_list': self.action_list, 'episode_number': self.curr_step}
    
    def get_reward(self):
      reward = [0]*self.states

      for i in range(self.states): 
        imp_feature = self.discrete_cc[i][self.affected_feature]
        action = self.action_list[i]

        # print(f"imp_feature: {imp_feature}")
        # print(f"action: {action}")

        reward[i] = 100 * np.exp(- ((action - imp_feature) ** 2) * 10000)

      # print("Reward distribution: ", reward)
      self.reward_list.append(reward)

      rew = sum(reward)
      print("Episode ",self.curr_step," Reward: ", reward)

      if(rew > self.min_reward):
        self.min_reward = rew
        self.min_reward_seq = reward
        self.best_actions = self.non_transformed
        # self.best_actions = self.action_list

      return rew
    
    def close(self):
      pass

# Discretize confirmed cases
def data_run(inp):

  print(f"Inside data_run: inp: {inp}")

  #Read data file given by user
  data = pd.read_csv('../PnP/dataset/abc.csv')

  # Inside data_run: inp: <QueryDict: {'filename': [''], 'episodes': ['30'], 'epochs': ['30'], 
  # 'total_resources': ['30'], 'resource': ['vaccine'], 'quantity': ['100'], 'states.value': ['Confirmed.cases'],
  # 'states.label': ['Confirmed.cases'], 'rewards.value': ['State...UT'], 'rewards.label': ['State...UT'], 
  # 'features.0.value': ['State...UT'], 'features.0.label': ['State...UT'], 'features.1.value': ['Confirmed.cases'], 
  # 'features.1.label': ['Confirmed.cases'], 'model.value': ['DQN'], 'model.label': ['DQN']}>

  resource = inp['resource']
  quantity = int(inp['quantity'])
  distribution_area = inp['rewards.value']
  affected_feature = inp['states.value']

  is_temporal = 'temporalAttribute.value' in list(inp.keys())
  if is_temporal:
    temporal_feature = inp['temporalAttribute.value']
    print(f"temporal_feature: {temporal_feature}")

  print(f"resource: {resource},\nquantity: {quantity}, \ndistribution_area: {distribution_area}, \naffected_feature: {affected_feature}")
  
  n=5
  features = []
  for i in range(n):
    features.append(inp[f"features.{i}.value"])


  print(f"num of unique distribution areas: {len(data[distribution_area].unique())}")

  cols = features
  if distribution_area not in features:
    cols.append(distribution_area)
  if affected_feature not in features:
    cols.append(affected_feature)

  affected_feature = cols.index(affected_feature)
  data = data[cols]
  
  # convert nominal to catagorical data
  datatypes = data.dtypes
  label_encoder = LabelEncoder() 
  nominal = [] 
  for i in range(len(datatypes)):
    if datatypes[i] == 'object':      # string values i.e. nominal data
      nominal.append(i)
      arr = data[data.columns[i]]
      arr= label_encoder.fit_transform(arr)  
      data[data.columns[i]] = arr

  # fill missing values for nominal data using mode
  mode_imputer= SimpleImputer(missing_values=np.nan, strategy='most_frequent')  

  for i in nominal:
    arr = data[data.columns[i]].to_numpy().reshape(-1, 1)
    imputer = mode_imputer.fit(arr)
    data[data.columns[i]] = imputer.transform(arr)  

  # fill missing values for rest of the data using mean
  mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
  st = MinMaxScaler()

  for i in range(len(cols)):
    if i not in nominal:
      arr = data[data.columns[i]].to_numpy().reshape(-1, 1)
      imputer = mode_imputer.fit(arr)
      data[data.columns[i]] = st.fit_transform(imputer.transform(arr))  

  print(data.head())
  print(data.info())


  if is_temporal:
    data = data.sort_values(temporal_feature)

  temp_imp = np.array(data)


  rewards = []
  best_rewards = []
  best_actions = []
  # # 15 hospitals
  no_states = len(data[distribution_area].unique())
  prev = 0
  no_episodes = inp['episodes']
  total_available = inp['total_resources']
  # each day
  for i in range(0,len(data),no_states):
    #Intialize matrix with values
    discrete_cc = []
    states = []

    for j in range(prev,prev+no_states):
      st = []
      for k in cols:
        st.append(float(data.loc[j,k]))
      states.append(st)
      discrete_cc.append((temp_imp[j]))
      
    prev += no_states
    # print("States: ",states)
    # print("Discrete: ",discrete_cc)
    
      
    # init environment - s=3, ep = 1000, total = 100
    env = StatesEnv(no_states,int(inp['epochs']),quantity,states, discrete_cc, affected_feature)
    
    print(env.st_ates)
    # ACKTR model with each episode = 10 timesteps and 100 episodes

    model = A2C("MlpPolicy", env, verbose=1).learn(total_timesteps=10)

    print("Best actions: ",env.best_actions * quantity,"\nBest reward: ", env.min_reward)
    rewards = env.reward_list
    best_rewards.append(env.min_reward_seq)
    best_actions.append(env.best_actions)
  
  cols = data.columns.values.tolist()
  data_np = np.array(data)
  cols.append('Action')
  cols.append('Reward')
  with open('acktr_data_output.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(cols)
    main_iter = 0
    sub_iter = 0
    
    for i in range(len(data_np)):
      row = []
      for j in range(len(cols)-2):
        row.append(data_np[i][j])
      row.append(best_actions[main_iter][sub_iter])
      row.append(best_rewards[main_iter][sub_iter])
      
      sub_iter += 1
      
      if (sub_iter == len(data[distribution_area].unique())):
        sub_iter = 0
        main_iter += 1

      # write a row to the csv file
      writer.writerow(row)
  return best_actions[-1] * quantity, best_rewards, 'acktr_data_output.csv'


