# -*- coding: utf-8 -*-
"""Discretized_RLPnP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XmP_ywqYP7MaGmsjjbUxE97KJV6Hpb2W
"""

import numpy as np 
import gym 
from gym import spaces 
import matplotlib.pyplot as plt 
import math
import random
from stable_baselines import PPO2, A2C, ACKTR
from stable_baselines.common.cmd_util import make_vec_env
from sklearn.preprocessing import KBinsDiscretizer
import pandas as pd
import csv
from sklearn.decomposition import NMF
import matplotlib.pyplot as plt

# Environment
class StatesEnv(gym.Env):
    """
    Customised Environment that follows gym interface.
    Describes relevant properties of the state and action spaces. 
    """

    # Human friendly output
    metadata = {'render.modes':['human']}
    
    
    # Initialize
    def __init__(self, s, episodes, total,states, discrete_cc):

        self.states = s #no of independent simulations to be run 
        
        # lower limit of confirmed and death
        low = np.array(s*[0,0]).reshape((s,2))
        # higher limit of confirmed and death
        high = np.array(s*[100,20]).reshape((s,2))

        # observation_space = states (grid)
        self.observation_space = spaces.Box(low, high, shape=(s, 2), dtype = np.float)

        # actions are vectors of the form [n1, n2, n3,...nk] for k states 
        # self.action_space = spaces.Box(low = np.zeros((2), dtype = int), high = np.array([100,100]), shape = (s,2), dtype = np.float)

        # Action space is vector
        self.action_space = spaces.Box(low = np.zeros(s, ),high = np.array([total]*s), shape=(s, ), dtype = np.float)

        # starts at step = 0
        self.curr_step = 0

        # done set to False at the beginning
        self.done = False
        
        self.valueMap = np.zeros((self.states, 100))
        # total number of resources available
        self.total = total #total number of vials available in 1 batch = batch size 
        
        self.fixed_total = total
        # number of episodes
        self.episodes = episodes
        # number of received resources by each state
        self.received = [0]*self.states
        self.states_cond = []
        self.action_list = []
        self.gamma = 0.20
        self.epsilon = 1.0
        # init min reward
        self.min_reward = -10000000
        self.min_reward_seq = []
        self.best_actions = []
        self.reward_list = []
        self.st_ates = states
        self.non_transformed = []

        self.discrete_cc = discrete_cc
        print("States: ",self.st_ates)
            
    def get_discrete_int(self, n):
        discrete_int = int(n)
        return discrete_int

    def reset(self):
        """
        Resets observation_space to a matrix initialising situation of states wrt the current figures; 
        action_space tp start exploring from the point of equal distribution between all states.
        """
        
        self.curr_step = 0
        self.cn = 0
        self.done = False
        self.total = self.fixed_total
        # Declare the Initial Conditions for the States
       
        self.states_cond =  np.array(self.st_ates)
        #store the actions in an array 
        self.action_list = np.array([self.fixed_total/(self.states)]*(self.states))

        return self.states_cond
        

    def step(self, action): 

        # check if we're done
        if self.curr_step >= self.episodes - 1:
            self.done = True
        # print("Are we done?", self.done)
        
        if(self.cn == self.episodes/10):
          self.cn = 0 
          self.epsilon = self.epsilon/10    
        if self.states_cond is None:
            raise Exception("You need to reset() the environment before calling step()!")
       
        #start with equal distribution 
        if self.curr_step == 1:
          self.action_list = np.array([self.fixed_total/(self.states)]*(self.states))
          self.reward_list = []
        else:
          self.action_list = action

        #exploration vs exploitation        
        if random.uniform(0, 1) < self.epsilon:
            for i in range(self.states):
              action[i] = np.random.randint(0, self.fixed_total)
            self.action_list = action
                        
        else:
            self.action_list = action

        # print ("Action List: ", self.action_list)

        # Proportion
        sum_of_actions = sum(self.action_list)
        if(sum_of_actions == 0):
          sum_of_actions = 1
        self.action_list = self.action_list/sum_of_actions
        self.non_transformed = self.action_list

        temp_actions = self.action_list.reshape((self.action_list.shape[0], 1))

        # Discretize actions
        est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
        est.fit(temp_actions)
        self.action_list = est.transform(temp_actions).reshape((self.action_list.shape[0], ))

        # print ("HIIII: ", self.action_list.shape)

        #update action_list to store only the most recently used action values 
        # self.action_list = action
        # print("Distribution set: ",self.action_list)

        #no of units distrbuted to respective states              
        for i in range(self.states):
            self.received[i] = self.action_list[i]

        #simulation
        for i in range(self.states):
          if(self.states_cond[i,0] >=0):
            self.states_cond[i,0] = self.states_cond[i,0]-1
             

        #reward only when task done 
        reward = self.get_reward()
        self.reward_list.append(reward)
        

        # increment episode
        self.curr_step += 1

        return self.states_cond, reward, self.done, {'action_list': self.action_list, 'episode_number': self.curr_step}
    
    def get_reward(self):
      reward = [0]*self.states

      for i in range(self.states): 
        imp_feature = self.discrete_cc[i]
        action = self.action_list[i]

        if ((imp_feature == 0 and action == 0) or (imp_feature == 1 and action == 1) or (imp_feature == 2 and action == 2)):
          reward[i] = 1

        elif ((imp_feature == 0 and action == 1) or (imp_feature == 1 and action == 0) or (imp_feature == 1 and action == 2)):
          reward[i] = 0

        elif ((imp_feature == 0 and action == 2) or (imp_feature == 2 and action == 0)):
          reward[i] = -1

      # print("Reward distribution: ", reward)

      rew = sum(reward)
      # print("Episode ",self.curr_step," Reward: ", rew)

      if(rew > self.min_reward):
        self.min_reward = rew
        self.min_reward_seq = reward
        self.best_actions = self.non_transformed
        # self.best_actions = self.action_list

      return rew
    
    def close(self):
      pass

# Discretize confirmed cases
def data_run(inp):
  print(f"Inside data_run: inp: {inp}")
  #Read data file given by user
  data = pd.read_csv(inp.file_path)
  num_states = len(data[inp.states].unique())

  print(f"num_states: {num_states}")

  #Get column names
  cols = data.columns.values.tolist()
  print(f"cols: {cols}")

  #Create an array with numerical data only
  remove_val = []
  remove_ind = []
  cn = 0

  for i in data.dtypes:
    cm = str(i)
    if cm != 'int64' and cm != 'float64':
      remove_val.append(cols[cn])
      remove_ind.append(cn)
    cn+=1
  

  df = data.drop(remove_val,axis=1)
  arr = df.to_numpy()

  #Change all negative values to 0
  for i in range(0,len(arr)):
    for j in range(0,len(arr[i])):
      if(arr[i][j]< 0):
        arr[i][j]=0


  #Find NMF value and normalize
  model = NMF(n_components = 1)
  W = model.fit_transform(arr)
  H = model.components_
  W = np.array(W)
  cn = 0
  for i in range(0,len(W)):
    if(cn == num_states):
      cn = 0
    if(cn == 0):
      sm = 0
      for j in range(0,num_states):
        sm+=W[i+j][0]
      for j in range(0,num_states):
        W[i+j][0] /= sm
    cn+=1
  df1 = np.concatenate((arr,W),axis=1)
  cols.append('Action Value')
  for i in remove_val:
    cols.remove(i)
  
  df1 = pd.DataFrame(df1, columns = cols)
  

  #Find feature with highest correlation
  corr = df1.corr()
  labels = cols
  values = []
  mx = -100
  ft = 0
  cn = 0
  for i in corr['Action Value']:
    values.append(i)
    if(mx < i and cols[cn] != 'Action Value'):
      mx = i
      ft = cn
    cn+=1
  

      
  #Plotting correlation values
  # fig = plt.figure(figsize = (10, 5))
  # # creating the bar plot
  # labels.pop()
  # values.pop()
  # plt.bar(labels, values,width = 0.4)
  # plt.xlabel("Features")
  # plt.ylabel("Correlation Values")
  # plt.show()


  temp_imp = np.array(data[cols[ft]])
  temp_imp = temp_imp.reshape((temp_imp.shape[0], 1))

  est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
  est.fit(temp_imp)
  temp_imp = est.transform(temp_imp)
  rewards = []
  best_rewards = []
  best_actions = []
  # 15 hospitals
  no_states = num_states
  prev = 0
  no_episodes = inp.episodes
  total_available = inp.total_resources
  # each day
  for i in range(0,len(data),num_states):
    #Intialize matrix with values
    discrete_cc = []
    states = []
    # all states data for that day
    for j in range(prev,prev+no_states):
      st = []
      for k in inp.attributes:
        st.append(float(data.loc[j,[k]]))
      states.append(st)
      discrete_cc.append((temp_imp[j]))
      
    prev += no_states
    print("States: ",states)
    print("Discrete: ",discrete_cc)
    
      
      # init environment - s=3, ep = 1000, total = 100
    env = StatesEnv(no_states,inp.epochs,inp.total_resources,states, discrete_cc)
    
  #   #print(env.st_ates)
  #   # ACKTR model with each episode = 10 timesteps and 100 episodes
    model = ACKTR('MlpPolicy', env, verbose=1).learn(inp.episodes)
    print("Best actions: ",env.best_actions,"\nBest reward: ", env.min_reward)
    rewards = env.reward_list
    best_rewards.append(env.min_reward_seq)
    best_actions.append(env.best_actions)
  
  cols = data.columns.values.tolist()
  data_np = np.array(data)
  cols.append('Action')
  cols.append('Reward')
  with open('acktr_data_output.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(cols)
    main_iter = 0
    sub_iter = 0
    
    for i in range(len(data_np)):
      row = []
      for j in range(len(cols)-2):
        row.append(data_np[i][j])
      row.append(best_actions[main_iter][sub_iter])
      row.append(best_rewards[main_iter][sub_iter])
      
      sub_iter += 1
      
      if (sub_iter == len(data[inp.states].unique())):
        sub_iter = 0
        main_iter += 1

      # write a row to the csv file
      writer.writerow(row)
  
  return best_actions, best_rewards, 'acktr_data_output.csv'

